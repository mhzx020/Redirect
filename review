1．_无监督和有监督的区别
无监督，直接对输入数据集进行建模，不需要训练
有监督，对有标记的训练样本进行训练，得出模型，通过模型对新数据进行分类

2．朴素贝叶斯算法
基于条件概率，主要思想是在A发生的基础上再发生B的概率等于在B发生的基础上再发生A的概率，我们要找出在一系列特征基础上属于某一个类的概率，可以通过这个类出现的情况下出现这一系列特征的概率而求出

3.  SVM
非线性映射是SVM的理论基础，SVM利用內积和函数代替向高维空间的非线性映射
目标是求出划分的最优超平面，泛化能力强，SVM最终的决策函数只由少数的支持向量所决定，计算复杂度取决于支持向量，某种程度上避免了“维数灾难”，SVM在小样本训练集上能够得到比其它算法好很多的结果，但不太适合于大样本数据

多分类问题可以通过构建多个SVM分类器的组合来解决，
一种是一对多，构建多个分类器，每次把其中一类分到一个集合，剩下的分到另外一个集合，进行测试的时候让测试样本通过全部分类器的测试，取所有分类结果中可能性最大的一个，缺点是因为训练集是1：M, 导致出现较大偏差

另外一种是一对一，就是在任意两个类之间构建一个分类器，总共要k(k-1)/2个分类器，最后进行投票

还有一种是层次分类法，每一次在两个大类当中进行划分，然后在选出的大类当中再一次进行划分直到得出一个单独的类别

核函数和惩罚参数

4.逻辑斯蒂回归
逻辑斯蒂回归是基于线性回归的，它的结果是把线性回归的结果映射到了sigmoid函数中，而sigmoid函数的输出结果是在0到1之间，可以很好的进行分类，比较成熟，训练时间短，对样本分布敏感，

5.损失函数
一般用来衡量均方误差（估计值-实际值）^2/2，即模型的准确性，损失函数越小，模型越准确

6.如何解决过拟合问题
1．重新清洗数据
2．加入正则化项，如L1、L2惩罚
3．增加训练数据
4．Dropout方法，在神经网络中以一定概率进行跳过神经元


7.解决数据不均衡问题
1. 采样，进行上采样，从小样本复制多份，为了避免过拟合可以在生成新数据时进行轻微的随机扰动，下采样，从大样本剔除部分，为了避免信息的损失可以进行多次下采样，从而训练多个不同的分类器，最终组合多个分类器的结果
2.数据合成，利用已有样本生成更多的样本，SMOTE利用小样本在特征空间的相似性来生成新样本，小样本Xi加上Xi与它的一个K近邻的差乘上一个随机数
3.加权，即对不同类别分错的代价不同
4.进行一分类或者异常检测


8. L1和L2
L1可以实现矩阵的稀疏，使得矩阵的大部分元素为0
L2可以防止模型过拟合，使得模型更简单，把L2放到损失函数中，当使得损失函数最小的同时也在使得L2最小，L2是权值向量的平方和再求平方根，当L2变小，说明有权重逐渐变为0，模型由高维降到低维

9. 如何解决欠拟合问题
1．添加其他特征项，可以通过组合，泛化和相关性来添加特征
2．添加多项式特征
3．减少正则化参数

10.随机森林
对样本进行随机的行采样和列采样从而生成多个不同的决策树，最终结果由多棵决策树投票得出
解决了决策树的过拟合问题，也可用于降维

11.GBDT
最初的Boost算法是在每一步训练结束后增加分错的点的权重，减少分对的点的权重，进行了N次迭代后得到N个简单的分类器然后组合起来投票

而GBDT每一次计算都是为了减少上一次的残差，每一个新的模型都是为了使之前模型的残差往梯度方向减少，防止过拟合

12.对多元函数的参数求偏导，把求得的各个参数的偏导以向量的形式写出来，就是梯度，梯度是函数变化增长最快的地方

13.如何进行特征选择
减少特征数量，降维，使泛化能力更强，增强特征和特征值之间的理解
去掉取值变化小的特征
单变量特征选择
计算pearson相关系数
随机森林

14异常检测


15.HOG,LDA,PCA
LDA和PCA都是将原有数据从高维映射到低维
而HOG则是将图像进行分割，然后对每个块计算HOG特征，然后累加


16.KNN
算距离，找最近的K个邻居，归类到主要类别中去

无需训练，适合对稀有事件进行分类，多分类
计算量大

17．决策树
最大深度
叶子节点的最小样本数
非叶子节点的最小可分割样本数


18.	原始数据可能存在的问题
	不一致；重复；含噪声；维度高


19.	使用数据的原则
	尽可能的赋予属性名和属性值明确的含义
	去除唯一属性
	去除重复性
	合理选择关联字段

20.	常见的数据预处理方法
	数据清洗：清除错误，噪声，将各种数据集统一起来，处理缺失值
	       缺失值处理的方法
		删除法：
（1）	删除观测样本
（2）	删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除
（3）	使用完整的原始数据进行分析
（4）	改变权重，删除缺失数据之后对完整数据按不同权重进行加工
		插补法：
		（1）计算缺失值所在变量所有非缺失观测值的均值，用均值来代替缺失			值
		（2）根据完整的样本以其他变量为自变量，插补变量为因变量建立回归			模型预测缺失值
		（3）热平台插补，在非缺失数据集中找到一个与缺失值所在样本相似的			样本，利用其中的观测值对缺失值进行插补



		（4）在实际操作中，尤其当变量数量很多时，通常很难找到与需要插补		样本完全相同的样本，此时可以按照某些变量将数据分层，在层中对缺		失值使用均值插补，即采取冷平台插补法。
	      噪声数据处理
		噪声检查的方法：
（1）	通过寻找数据集中与其他观测值及均值差距最大的点作为异常
（2）	聚类方法检测，离群的点即为异常点
		去除噪声的方法
			分箱、聚类、回归等 
	数据平滑：
		按平均值平滑：对一箱中的数据用平均值来代替
		按边界值平滑：用距离较小的边界值来代替箱中的数据
		按中值平滑：取箱子的中值来替代箱中的所有值

	数据集成：将多个数据源中的数据合并，并放到一个一致的数据存储中
		删除数据冗余
	数据变换：找到数据的特征表示，用维度变换来减少有效变量的数目或找到数	据的不变式，包括规格化(使单位一致)、投影等
		光滑：去掉噪声
		属性构造：由给定的属性构造出新属性并添加到数据集中
		聚集：对数据进行汇总
		规范化：比如数据标准化处理
		离散化：将定量数据转化为定性

	数据规约：在尽可能保持数据原貌的前提下，最大限度地精简数据量，有两个	途径：属性选择和数据采样
		常用方法有：分类树、随机森林、主成分分析


21.特征工程：
	本质上来说，呈现给算法的数据应该能拥有基本数据的相关结构或属性。当你做特征工程时，其实是将数据属性转换为数据特征的过程，属性代表了数据的所有维度，在数据建模时，如果对原始数据的所有属性进行学习，并不能很好的找到数据的潜在趋势，而通过特征工程对你的数据进行预处理的话，你的算法模型能够减少受到噪声的干扰，这样能够更好的找出趋势。事实上，好的特征甚至能够帮你实现使用简单的模型达到很好的效果。
	交叉特征，即组合特征
	




特征选择：
	过滤式：先进行特征选择，再训练模型
	包裹式：产生特征子集，根据子集训练模型，不断重复这个过程
	嵌入式：在模型训练中自动进行了特征选择

模型融合方法：
	线性加权融合
	交叉融合，其思路是在推荐结果中，穿插不同推荐模型的结果，以确保结果的	多样性
	瀑布融合，通过将不同粒度的过滤器前后衔接的方法来进行
	通过使用不同的数据来源，抽取不同的特征，输入到推荐模型中进行训练，然	后将结果合并
	预测融合法，把多个模型的预测结果作为特征再次进行训练
	加性融合：类似于GDBT

	


